# Work Logbook - ROB6323 Go2 Project

This file tracks all modifications made to the baseline repository for the Go2 quadruped locomotion project.

---

## 2025-12-18 - Initial Setup

### Changes Made

#### 1. Contact Sensor Registration
**File:** `rob6323_go2_env.py`
```python
# Line 71
self.scene.sensors["contact_sensor"] = self._contact_sensor
```
- **Purpose:** Properly register the contact sensor with the scene for force data tracking
- **Context:** Required for implementing contact-based rewards and terminations

#### 2. Base Height Termination Threshold
**File:** `rob6323_go2_env_cfg.py`
```python
# Line 84
base_height_min = 0.05
```
- **Purpose:** Lowered termination threshold from 0.20m to 0.05m to allow robot to crouch lower during learning
- **Context:** Part of early termination criteria to speed up training and encourage elevated base

---

## 2025-12-18 - Tutorial Implementation (Parts 1-6)

### Part 1: Action Rate Penalties
**File:** `rob6323_go2_env.py`
```python
# Lines 58-64: Action history buffer
self.last_actions = torch.zeros(self.num_envs, gym.spaces.flatdim(self.single_action_space), 3,
                                dtype=torch.float, device=self.device, requires_grad=False)

# Lines 183-187: Action rate penalties (first and second derivatives)
rew_action_rate = torch.sum(torch.square(self._actions - self.last_actions[:, :, 0]), dim=1) * (self.cfg.action_scale ** 2)
rew_action_rate += torch.sum(torch.square(self._actions - 2 * self.last_actions[:, :, 0] + self.last_actions[:, :, 1]), dim=1) * (self.cfg.action_scale ** 2)
```

**File:** `rob6323_go2_env_cfg.py`
```python
# Line 100
action_rate_reward_scale = -0.1
```
- **Purpose:** Penalize jerky motions to encourage smooth actuator commands

### Part 2: Low-Level PD Controller
**File:** `rob6323_go2_env.py`
```python
# Lines 38-42: PD control parameters
self.Kp = self.cfg.Kp
self.Kd = self.cfg.Kd
self.torque_limits = self.cfg.torque_limits

# Lines 105-120: Manual PD control in _apply_action()
torques = torch.clamp(
    self.Kp * (
        self.desired_joint_pos
        - self.robot.data.joint_pos
    )
    - self.Kd * self.robot.data.joint_vel,
    -self.torque_limits,
    self.torque_limits,
)
self.robot.set_joint_effort_target(torques)
```

**File:** `rob6323_go2_env_cfg.py`
```python
# Lines 36-39: PD gains
Kp = 20.0
Kd = 0.5
torque_limits = 100.0

# Lines 70-76: Disable implicit actuator
robot_cfg.actuators["base_legs"] = ImplicitActuatorCfg(
    joint_names_expr=[".*_hip_joint", ".*_thigh_joint", ".*_calf_joint"],
    effort_limit=23.5,
    velocity_limit=30.0,
    stiffness=0.0,  # Disabled
    damping=0.0,    # Disabled
)
```
- **Purpose:** Full control over gains and torque limits instead of using implicit controller

### Part 3: Early Termination (Base Height Min)
**File:** `rob6323_go2_env.py`
```python
# Lines 254-256: Base height check in _get_dones()
base_height = self.robot.data.root_pos_w[:, 2]
cstr_base_height = base_height < self.cfg.base_height_min
terminated = cstr_termination_contacts | cstr_upsidedown | cstr_base_height
```

**File:** `rob6323_go2_env_cfg.py`
```python
# Line 108
base_height_min = 0.05
```
- **Purpose:** Terminate episodes early if robot collapses (base < 5cm)

### Part 4: Raibert Heuristic (Gait Shaping)
**File:** `rob6323_go2_env_cfg.py`
```python
# Line 29
observation_space = 52  # 48 base + 4 clock inputs

# Lines 32-34
raibert_heuristic_reward_scale = -1.0
feet_clearance_reward_scale = -30.0
tracking_contacts_shaped_force_reward_scale = 0.4
```

**File:** `rob6323_go2_env.py`
```python
# Lines 83-88: Gait state variables
self.gait_indices = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
self.clock_inputs = torch.zeros(self.num_envs, 4, dtype=torch.float, device=self.device)
self.desired_contact_states = torch.zeros(self.num_envs, 4, dtype=torch.float, device=self.device)

# Line 189-190: Update gait and compute Raibert reward
self._step_contact_targets()
rew_raibert_heuristic = self._reward_raibert_heuristic()

# Lines 436-469: Raibert heuristic implementation (foot placement guidance)
def _reward_raibert_heuristic(self):
    # Convert footsteps to body frame
    cur_footsteps_translated = self.foot_positions_w - self.robot.data.root_pos_w.unsqueeze(1)
    # ... (calculates desired foot positions based on velocity commands)
    err_raibert_heuristic = torch.abs(desired_footsteps_body_frame - footsteps_in_body_frame[:, :, 0:2])
    reward = torch.sum(torch.square(err_raibert_heuristic), dim=(1, 2))
    return reward
```
- **Purpose:** Use Raibert heuristic as "teacher" to encourage proper stepping patterns

### Part 5: Refined Reward Function

#### Part 5.1: Configuration
**File:** `rob6323_go2_env_cfg.py`
```python
# Lines 102-105
orient_reward_scale = -5.0
lin_vel_z_reward_scale = -0.02
dof_vel_reward_scale = -0.0001
ang_vel_xy_reward_scale = -0.001
```

#### Part 5.2: Implementation
**File:** `rob6323_go2_env.py`
```python
# Lines 196-206: 4 stability reward terms
rew_orient = torch.sum(torch.square(self.robot.data.projected_gravity_b[:, :2]), dim=1)
rew_lin_vel_z = torch.square(self.robot.data.root_lin_vel_b[:, 2])
rew_dof_vel = torch.sum(torch.square(self.robot.data.joint_vel), dim=1)
rew_ang_vel_xy = torch.sum(torch.square(self.robot.data.root_ang_vel_b[:, :2]), dim=1)

# Lines 234-237: Integrated into rewards dictionary
"orient": rew_orient * self.cfg.orient_reward_scale,
"lin_vel_z": rew_lin_vel_z * self.cfg.lin_vel_z_reward_scale,
"dof_vel": rew_dof_vel * self.cfg.dof_vel_reward_scale,
"ang_vel_xy": rew_ang_vel_xy * self.cfg.ang_vel_xy_reward_scale,
```
- **Purpose:** Shape natural, stable locomotion behavior

### Part 6: Advanced Foot Interaction

**File:** `rob6323_go2_env.py`
```python
# Lines 208-213: Foot clearance reward
phases = 1 - torch.abs(1.0 - torch.clip((self.foot_indices * 2.0) - 1.0, 0.0, 1.0) * 2.0)
foot_height = self.foot_positions_w[:, :, 2]
target_height = 0.08 * phases + 0.02
rew_foot_clearance = torch.square(target_height - foot_height) * (1 - self.desired_contact_states)
rew_feet_clearance = torch.sum(rew_foot_clearance, dim=1)

# Lines 216-223: Contact force tracking reward
foot_forces = torch.norm(self._contact_sensor.data.net_forces_w[:, self._feet_ids_sensor, :], dim=-1)
rew_tracking_contacts_shaped_force = torch.zeros(self.num_envs, device=self.device)
for i in range(4):
    rew_tracking_contacts_shaped_force += -(1 - self.desired_contact_states[:, i]) * (
        1 - torch.exp(-1 * foot_forces[:, i] ** 2 / 100.0)
    )
rew_tracking_contacts_shaped_force = rew_tracking_contacts_shaped_force / 4
```

**File:** `rob6323_go2_env_cfg.py`
```python
# Lines 33-34
feet_clearance_reward_scale = -30.0
tracking_contacts_shaped_force_reward_scale = 0.4
```
- **Purpose:** Encourage proper foot lifting during swing and light contact during stance

---

## Debugging Fixes

### Fix 1: Missing numpy import (Job 134821, 134834)
**Error:** `NameError: name 'np' is not defined` at line 399

**File:** `rob6323_go2_env.py`
```python
# Line 11: Added missing import
import numpy as np

# Line 399: Usage in clock input calculation
self.clock_inputs[:, 0] = torch.sin(2 * np.pi * foot_indices[0])
```

### Fix 2: Tensor shape mismatch (Job 134834, 134838)
**Error:** `RuntimeError: stack expects each tensor to be equal size, but got [4096] at entry 0 and [4096, 1] at entry 10`

**File:** `rob6323_go2_env.py`
```python
# Line 227: Added .squeeze() to fix shape
base_contact_force_magnitude = torch.max(torch.norm(net_contact_forces[:, :, self._base_id], dim=-1), dim=1)[0].squeeze()
```
**Context:** `torch.max(..., dim=1)[0]` returned `[4096, 1]` instead of `[4096]`

---

## Summary of Implementation

**Tutorial Completion:** Parts 1-6 fully implemented
**Files Modified:**
- `rob6323_go2_env.py` - Environment logic and reward calculations
- `rob6323_go2_env_cfg.py` - Configuration parameters and reward scales

**Total Reward Terms:** 13
1. Linear velocity tracking (XY)
2. Angular velocity tracking (Yaw)
3. Action rate penalties (smoothness)
4. Raibert heuristic (foot placement)
5. Orientation (body tilt)
6. Linear velocity Z (bouncing)
7. Joint velocities (fast motion)
8. Angular velocity XY (roll/pitch)
9. Feet clearance (swing height)
10. Contact force tracking (stance grounding)
11. Base collision penalty
12. Foot slip (horizontal foot velocity during stance)

---

## Notes
- Following tutorial at: `tutorial/tutorial.md`
- Only modifying: `rob6323_go2_env.py` and `rob6323_go2_env_cfg.py`
- NOT modifying: `rsl_rl_ppo_cfg.py` (PPO hyperparameters off-limits per project instructions)

---

## Next Steps
- [x] Implement action rate penalties (Part 1 of tutorial)
- [x] Add low-level PD controller (Part 2 of tutorial)
- [x] Implement early termination (Part 3 of tutorial)
- [x] Implement Raibert Heuristic rewards (Part 4 of tutorial)
- [x] Add refined reward terms (Part 5 of tutorial)
- [x] Implement advanced foot interaction rewards (Part 6 of tutorial)
- [x] Test training on Greene cluster (Run_01 completed)
- [ ] Analyze TensorBoard metrics and tune reward scales if needed
- [ ] Consider domain randomization for sim-to-real transfer

---

## Training Runs

### Run_01: Baseline (Job ID: 134652)
**Date:** 2025-12-18 09:39:23
**Status:** âœ… Completed
**Duration:** ~500 iterations

**Command:**
```bash
./train.sh
# Executes: python scripts/rsl_rl/train.py --task=Template-Rob6323-Go2-Direct-v0 --headless
```

**Configuration:**
- **Seed:** 42
- **Environment:** Baseline configuration (before tutorial implementation)
- **Observation Space:** 48 (no clock inputs yet)
- **Action Space:** 12
- **Num Envs:** 4096
- **Episode Length:** 20s
- **Reward Terms:** Only velocity tracking (lin_vel, yaw_rate)
  - `lin_vel_reward_scale = 1.0`
  - `yaw_rate_reward_scale = 0.5`
- **Actuator:** Implicit PD controller (stiffness=25.0, damping=0.5)
- **Termination:** Base contact, upside-down only

**PPO Hyperparameters:**
- Learning rate: 0.0003 (adaptive schedule)
- Gamma: 0.99
- Lambda (GAE): 0.95
- Mini-batches: 4
- Learning epochs: 5
- Steps per env: 32
- Policy network: [256, 128, 64] (ELU activation)
- Value network: [256, 128, 64] (ELU activation)

**Results:**
- Checkpoints saved: model_0.pt through model_499.pt (every 50 iterations)
- Final model: `model_499.pt`
- Exported policy: `policy.pt`, `policy.onnx`
- Evaluation video: `videos/play/rl-video-step-0.mp4`
- TensorBoard logs: `events.out.tfevents.1766068784.b-31-1.37.0`

**Notes:**
- Used baseline implementation with minimal reward shaping
- No action smoothing, posture stabilization, or foot interaction rewards
- Serves as performance baseline for comparison with enhanced implementations

**Logs Location:** `.logs/134652/rsl_rl/go2_flat_direct/2025-12-18_09-39-23/`

---

### Run_02: Full Implementation (Job ID: 134838)
**Date:** 2025-12-18 18:04:49
**Status:** âŒ Failed (Reward Imbalance)
**Duration:** 500 iterations

**Command:**
```bash
./train.sh
# Executes: python scripts/rsl_rl/train.py --task=Template-Rob6323-Go2-Direct-v0 --headless
```

**Configuration:**
- **Seed:** 42
- **Environment:** Full tutorial implementation (Parts 1-6) + collision penalties
- **Observation Space:** 52 (added 4 clock inputs for gait)
- **Action Space:** 12
- **Num Envs:** 4096
- **Episode Length:** 20s
- **Reward Terms:** 11 terms
  - Velocity tracking (2 terms)
  - Action smoothing (1 term)
  - Posture stabilization (4 terms: orient, lin_vel_z, dof_vel, ang_vel_xy)
  - Gait shaping (1 term: Raibert heuristic)
  - Foot interaction (2 terms: clearance, contact force tracking)
  - Collision penalty (1 term: base contact penalty)
- **Actuator:** Manual PD controller (Kp=20.0, Kd=0.5, torque_limits=100.0)
- **Termination:** Base contact, upside-down, base height < 0.05m

**Implementation Details:**
- **File:** `rob6323_go2_env_cfg.py`
  - Line 111: `base_collision_penalty_scale = -1.0`
- **File:** `rob6323_go2_env.py`
  - Line 11: Added `import numpy as np` (Fix 1)
  - Line 61: Added "base_collision_penalty" logging
  - Line 215: Added `.squeeze()` to collision penalty (Fix 2)
  - Lines 213-216: Collision penalty calculation
    - Penalizes base contact forces above 0.5N threshold
    - Uses clamped linear penalty scaled by contact force magnitude

**Results:**
- **Mean Reward:** -252.64 (vs Run_01: +29.52)
- **Velocity Tracking Performance:**
  - Lin vel XY: 0.10 (vs Run_01: 0.98) â†’ **90% degradation**
  - Ang vel Z: 0.09 (vs Run_01: 0.49) â†’ **81% degradation**
- **Penalty Breakdown:**
  - Contact force tracking: -6.37 (largest penalty)
  - Raibert heuristic: -3.83
  - Action rate: -1.70
  - Feet clearance: -0.55
  - Orient: -0.11
  - Others: minimal

**Analysis:**
- **Primary Issue:** Reward scale imbalance
  - Tracking rewards: ~1.5 total positive potential
  - Penalties: ~12.5 total negative accumulated
  - Ratio: 8:1 penalties overwhelming tracking
- **Robot Behavior:** Learned "do nothing" policy
  - Standing still minimizes penalties
  - Fails primary objective (velocity tracking)
  - Episodes reach full length (999 steps) without crashes
- **Root Cause:** Penalty scales (especially contact_force: 4.0, Raibert: -10.0, feet_clearance: -30.0) too aggressive relative to tracking rewards

**Logs Location:** `.logs/Run_02/rsl_rl/go2_flat_direct/2025-12-18_18-04-49/`

---

### Run_03: Rebalanced Rewards (Job ID: 134880)
**Date:** 2025-12-18 19:06:03
**Status:** âŒ Failed (Insufficient Rebalancing)
**Duration:** 500 iterations

**Command:**
```bash
./train.sh
# Executes: python scripts/rsl_rl/train.py --task=Template-Rob6323-Go2-Direct-v0 --headless
```

**Objective:** Fix reward imbalance from Run_02 by reducing the 2 largest penalty scales

**Configuration:**
- **Seed:** 42

**Changes from Run_02:**
- **File:** `rob6323_go2_env_cfg.py`
  - Line 32: `raibert_heuristic_reward_scale = -1.0` (was -10.0, 10x reduction)
  - Line 34: `tracking_contacts_shaped_force_reward_scale = 0.4` (was 4.0, 10x reduction)

**Unchanged from Run_02:**
- `feet_clearance_reward_scale = -30.0`
- `action_rate_reward_scale = -0.1`
- All posture stability penalties (orient, lin_vel_z, dof_vel, ang_vel_xy)
- Collision penalty, tracking rewards, PD controller, observation space

**Results:**
- **Mean Reward:** -62.34 (improved from Run_02: -252.64, but still negative)
- **Velocity Tracking Performance:**
  - Lin vel XY: 0.11 (vs Run_01: 0.98) â†’ **89% degradation** (marginal improvement from Run_02's 90%)
  - Ang vel Z: 0.14 (vs Run_01: 0.49) â†’ **73% degradation** (improved from Run_02's 81%)

**Analysis:**
- **Primary Issue:** 10x penalty reduction wasn't enough
  - Robot still learned conservative "do nothing" policy
  - Penalties still discouraging locomotion despite reduction
  - Tracking rewards (lin_vel: 1.0, yaw: 0.5) provide insufficient gradient
- **Root Cause:** Need stronger **positive incentives** to move, not just smaller penalties
  - Current positive potential: ~1.5 total
  - Remaining penalties still significant (clearance: -30.0, contact: 0.4, etc.)
- **Conclusion:** Reducing penalties alone is insufficient - must boost tracking rewards

**Logs Location:** `.logs/Run_03/rsl_rl/go2_flat_direct/2025-12-18_19-06-03/`

---

### Run_04: Boosted Tracking Rewards (Job ID: 134935)
**Date:** 2025-12-18 20:30:01
**Status:** âœ… Completed
**Duration:** 500 iterations

**Command:**
```bash
./train.sh
# Executes: python scripts/rsl_rl/train.py --task=Template-Rob6323-Go2-Direct-v0 --headless
```

**Objective:** Fix reward imbalance by boosting tracking rewards instead of further reducing penalties

**Strategy:** Provide stronger positive gradient toward velocity tracking

**Configuration:**
- **Seed:** 42

**Changes from Run_03:**

**File:** `rob6323_go2_env_cfg.py`
```python
# Line 98-99: Boosted tracking rewards
lin_vel_reward_scale = 3.0  # was 1.0
yaw_rate_reward_scale = 1.5  # was 0.5

# Line 106: Added foot slip penalty
foot_slip_reward_scale = -0.1
```

**File:** `rob6323_go2_env.py`
```python
# Lines 65-75: Added performance metrics tracking
self._episode_metrics = {
    key: torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
    for key in [
        "lin_vel_error",
        "ang_vel_error",
        "base_orient_error",
        "torque_squared",
    ]
}
self._episode_steps = torch.zeros(self.num_envs, dtype=torch.int, device=self.device)

# Lines 230-232: Foot slip penalty
foot_velocities = self.robot.data.body_vel_w[:, self._feet_ids, :2]
rew_foot_slip = torch.sum(torch.norm(foot_velocities, dim=-1) * self.desired_contact_states, dim=1)

# Lines 247-253: Accumulate metrics per step
self._episode_metrics["lin_vel_error"] += torch.sqrt(lin_vel_error)
self._episode_metrics["ang_vel_error"] += torch.sqrt(yaw_rate_error)
self._episode_metrics["base_orient_error"] += torch.acos(torch.clamp(self.robot.data.projected_gravity_b[:, 2], -1.0, 1.0))
torque_squared = torch.sum(torch.square(self.robot.data.applied_torque), dim=1)
self._episode_metrics["torque_squared"] += torque_squared
self._episode_steps += 1

# Lines 301-310: Log metrics to TensorBoard
extras = dict()
episode_steps = self._episode_steps[env_ids].float()
for key in self._episode_metrics.keys():
    episodic_metric_avg = torch.mean(self._episode_metrics[key][env_ids] / episode_steps)
    extras["Metrics/" + key] = episodic_metric_avg.item()
    self._episode_metrics[key][env_ids] = 0.0
extras["Metrics/episode_length"] = torch.mean(episode_steps).item()
self._episode_steps[env_ids] = 0
self.extras["log"].update(extras)
```

**Unchanged from Run_03:**
- All penalty scales (Raibert: -1.0, contact_force: 0.4, clearance: -30.0, etc.)
- All posture stability penalties
- PD controller, observation space, termination criteria

**New Metrics (TensorBoard):**
- `Metrics/lin_vel_error` - L2 velocity tracking error (m/s)
- `Metrics/ang_vel_error` - Angular velocity tracking error (rad/s)
- `Metrics/base_orient_error` - Orientation deviation from upright (rad)
- `Metrics/torque_squared` - Energy proxy (NmÂ²)
- `Metrics/episode_length` - Steps survived before termination

**Expected Impact:**
- New positive reward potential: ~4.5 (was ~1.5)
- Stronger gradient toward velocity tracking objectives
- Penalties remain meaningful but don't dominate
- Robot should pursue locomotion while experiencing smoothness penalties
- Motion should be functional + smoother than Run_01 baseline
- New metrics provide quantitative benchmarks for comparing smoothness and efficiency

**Logs Location:** `.logs/Run_04/rsl_rl/go2_flat_direct/2025-12-18_20-30-01/`

**Results:**
- **Mean Reward:** -11.28 (improved from Run_03: -62.34)
- **Status:** âœ… Training completed successfully, zero crashes
- **Velocity Tracking Performance:**
  - Lin vel XY: 2.701 (vs Run_01: 0.983) â†’ **175% improvement**
  - Ang vel Z: 0.473 (vs Run_01: 0.492) â†’ **96% of baseline performance maintained**
- **Stability Metrics:**
  - Base contact: 0.00 (no crashes, vs Run_01: also minimal)
  - Episode length: 999.0 (full episodes completed)
- **Penalty Breakdown:**
  - `tracking_contacts_shaped_force`: -1.517 (largest remaining penalty)
  - `rew_action_rate`: -0.906
  - `rew_raibert_heuristic`: -0.262
  - `feet_clearance`: -0.201 (performing well)
  - All others: < 0.1

**Analysis:**
- **Success:** 3x tracking reward boost successfully eliminated training crashes
  - Robot now pursues velocity objectives instead of "do nothing" policy
  - Zero base collisions demonstrate stable locomotion learned
  - Velocity tracking performance exceeds baseline by 175%
- **Remaining Bottleneck:** Contact force penalty (-1.517) is now largest penalty
  - Was 0.4 scale, designed to encourage soft foot landings during swing
  - Current scale may be too aggressive now that tracking rewards dominate
  - Feet clearance (-0.201) performing well, no further reduction needed
- **Mean Reward Still Negative:** Despite functional locomotion, overall -11.28
  - Positive potential: ~4.5/step from tracking
  - Contact force penalty accumulates faster than anticipated
  - Suggests room for further improvement

**Conclusion:** Run_04 validates the tracking boost strategy. Locomotion is functional and crash-free, but contact force penalty needs reduction to push mean reward positive.

---

### Run_05: Reduced Contact Force Penalty (Job ID: 135062)
**Date:** 2025-12-18
**Status:** ðŸ”„ Running
**Duration:** ~30 minutes
**Objective:** Push mean reward positive by reducing contact force penalty bottleneck identified in Run_04

**Legacy Job IDs:** 134990 (failed - FileNotFoundError), 135007 (failed - FileNotFoundError)

**Strategy:** Reduce largest remaining penalty (tracking_contacts_shaped_force) by 2x

**Configuration:**
- **Seed:** 42

**Changes from Run_04:**

**File:** `rob6323_go2_env_cfg.py`
```python
# Line 34: Reduce contact force penalty
tracking_contacts_shaped_force_reward_scale = 0.2  # was 0.4
```

**Unchanged from Run_04:**
- All tracking rewards (lin_vel: 3.0, yaw: 1.5)
- All other penalties (Raibert: -1.0, clearance: -30.0, action_rate: -0.1, etc.)
- PD controller, observation space, termination criteria (base_height_min: 0.05)

**Expected Impact:**
- Contact force penalty reduced from -1.517 to ~-0.76
- Net positive potential: ~4.5 tracking vs ~3.5 total penalties
- Expected mean reward: +10 to +20 range (positive territory)
- Maintains soft landing encouragement but less aggressively
- Should preserve Run_04's crash-free performance while improving overall reward

**Rationale:**
- Run_04 confirmed feet_clearance (-0.201) doesn't need reduction
- Contact force is isolated bottleneck at -1.517
- All other penalties < 1.0
- 2x reduction aligns contact penalty with other regularization terms
- Conservative change maintains tutorial structure

---

### Run_06: Reduced Action Rate + Feet Clearance Penalties (Job ID: TBD)
**Date:** 2025-12-18
**Status:** â³ Pending submission
**Duration:** ~30 minutes
**Objective:** Test combined penalty reduction on action smoothness and feet clearance

**Legacy Job IDs:** 134996 (failed - FileNotFoundError), 135012 (failed - FileNotFoundError)

**Strategy:** Reduce both action_rate and feet_clearance penalties by 2x

**Configuration:**
- **Seed:** 42

**Changes from Run_04:**

**File:** `rob6323_go2_env_cfg.py`
```python
# Line 33: Reduce feet clearance penalty
feet_clearance_reward_scale = -15.0  # was -30.0

# Line 100: Reduce action rate penalty
action_rate_reward_scale = -0.05  # was -0.1
```

**Unchanged from Run_04:**
- All tracking rewards (lin_vel: 3.0, yaw: 1.5)
- Other gait penalties (Raibert: -1.0, contact_force: 0.4)
- All posture penalties (orient, lin_vel_z, dof_vel, ang_vel_xy)
- PD controller, observation space, termination criteria (base_height_min: 0.05)

**Expected Impact:**
- Action rate penalty reduced from -0.906 to ~-0.45
- Feet clearance penalty reduced from -0.201 to ~-0.10
- Total penalties: ~2.95/step (vs Run_04: ~3.6/step)
- Net positive potential: ~4.5 tracking vs ~2.95 penalties
- Expected mean reward: Better than Run_04, potentially positive
- May allow more aggressive actions and natural foot lifting
- Risk: Potential increase in action jerkiness or excessive foot height

**Rationale:**
- Tests combined reduction of #2 and #4 penalty contributors from Run_04
- Action_rate (-0.906) + feet_clearance (-0.201) = -1.107 total reduction potential
- Run_04 showed feet_clearance was small but still present
- Combined test explores whether both constraints were over-tuned
- More aggressive than Run_05 (single parameter) but maintains all tutorial components

**Comparison to Run_05:**
- Run_05: Reduces contact_force only (largest penalty: -1.517 â†’ ~-0.76)
- Run_06: Reduces action_rate + feet_clearance (combined: -1.107 â†’ ~-0.55)
- Run_05 targets single largest bottleneck
- Run_06 targets two smaller bottlenecks simultaneously
- Results will reveal whether single large penalty or multiple small penalties are more critical

---

### Run_07: Higher Base Height Termination (Job ID: 135037)
**Date:** 2025-12-18
**Status:** ðŸ”„ Running
**Duration:** ~30 minutes
**Objective:** Test effect of stricter base height termination on posture quality

**Legacy Job IDs:** 135002 (failed - FileNotFoundError), 135015 (failed - FileNotFoundError)

**Strategy:** Same as Run_05 but with 4x higher base_height_min termination threshold

**Configuration:**
- **Seed:** 42

**Changes from Run_05:**

**File:** `rob6323_go2_env_cfg.py`
```python
# Line 109: Increase base height termination threshold
base_height_min = 0.2  # was 0.05
```

**Unchanged from Run_05:**
- All tracking rewards (lin_vel: 3.0, yaw: 1.5)
- All penalties (contact_force: 0.2, action_rate: -0.1, clearance: -30.0, Raibert: -1.0, etc.)
- PD controller, observation space

**Expected Impact:**
- Episodes terminate if base drops below 20cm (vs 5cm in Run_05)
- Encourages robot to maintain upright posture
- May increase early terminations during initial learning
- Could improve base stability metrics (pitch, roll, height)
- Risk: Harder exploration, longer training time

**Rationale:**
- Run_04 showed perfect base contact avoidance (0.00)
- Original threshold (0.05m) may be too lenient
- Stricter threshold forces robot to learn better posture
- Tests whether termination criteria can act as implicit reward shaping
- Compares to Run_05 to isolate effect of termination threshold alone
